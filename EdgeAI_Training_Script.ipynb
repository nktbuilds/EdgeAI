{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a001970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_FOLDER = \"MHOOP_dataset\"\n",
    "OUTPUT_FOLDER = \"MHOOP_outputs\"\n",
    "WINDOW_SIZE = 150         # 3 seconds @ 50Hz\n",
    "STEP_SIZE = 75            # 50% overlap\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "    print(f\"Created output directory: {OUTPUT_FOLDER}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADING\n",
    "# ==========================================\n",
    "def load_and_merge_data(folder_path):\n",
    "    all_data = []\n",
    "    db_files = glob.glob(os.path.join(folder_path, \"*.db\"))\n",
    "    \n",
    "    if not db_files:\n",
    "        print(\"No .db files found! Make sure your files are in the 'datasets' folder.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found {len(db_files)} database files.\")\n",
    "\n",
    "    for db_file in db_files:\n",
    "        filename = os.path.basename(db_file)\n",
    "        label = os.path.splitext(filename)[0]\n",
    "        \n",
    "        print(f\"Loading '{filename}' as class: '{label}'...\")\n",
    "        \n",
    "        try:\n",
    "            conn = sqlite3.connect(db_file)\n",
    "            df = pd.read_sql_query(\"SELECT x, y, z FROM accelerometer_data ORDER BY local_timestamp\", conn)\n",
    "            conn.close()\n",
    "            \n",
    "            df['label'] = label\n",
    "            all_data.append(df)\n",
    "            print(f\"  -> Loaded {len(df)} samples.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  -> Error loading {db_file}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "    return full_df\n",
    "\n",
    "# Load the data\n",
    "print(\"--- STARTING DATA LOAD ---\")\n",
    "data = load_and_merge_data(DATA_FOLDER)\n",
    "\n",
    "# ==========================================\n",
    "# 2. PREPROCESSING & SPLITTING\n",
    "# ==========================================\n",
    "def create_windows_from_array(data_array, label, window_size, step_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    # Loop over the array to create windows\n",
    "    for i in range(0, len(data_array) - window_size + 1, step_size):\n",
    "        window = data_array[i : i + window_size]\n",
    "        X.append(window)\n",
    "        y.append(label)\n",
    "    return X, y\n",
    "\n",
    "def robust_time_split(df, window_size, step_size, val_ratio=0.15, test_ratio=0.15):\n",
    "    X_train_list, y_train_list = [], []\n",
    "    X_val_list, y_val_list = [], []\n",
    "    X_test_list, y_test_list = [], []\n",
    "\n",
    "    # Process each class separately\n",
    "    for label, group in df.groupby('label'):\n",
    "        sensor_data = group[['x', 'y', 'z']].values\n",
    "        total_len = len(sensor_data)\n",
    "        \n",
    "        # Calculate split indices based on time\n",
    "        # Train ends where Val begins\n",
    "        train_end_idx = int(total_len * (1 - val_ratio - test_ratio))\n",
    "        # Val ends where Test begins\n",
    "        val_end_idx = int(total_len * (1 - test_ratio))\n",
    "        \n",
    "        # Split raw data by time\n",
    "        train_segment = sensor_data[:train_end_idx]\n",
    "        val_segment = sensor_data[train_end_idx:val_end_idx]\n",
    "        test_segment = sensor_data[val_end_idx:]\n",
    "        \n",
    "        # Create windows for each segment\n",
    "        w_train, l_train = create_windows_from_array(train_segment, label, window_size, step_size)\n",
    "        w_val, l_val = create_windows_from_array(val_segment, label, window_size, step_size)\n",
    "        w_test, l_test = create_windows_from_array(test_segment, label, window_size, step_size)\n",
    "        \n",
    "        X_train_list.extend(w_train)\n",
    "        y_train_list.extend(l_train)\n",
    "        X_val_list.extend(w_val)\n",
    "        y_val_list.extend(l_val)\n",
    "        X_test_list.extend(w_test)\n",
    "        y_test_list.extend(l_test)\n",
    "\n",
    "    return (np.array(X_train_list), np.array(y_train_list), \n",
    "            np.array(X_val_list), np.array(y_val_list),\n",
    "            np.array(X_test_list), np.array(y_test_list))\n",
    "\n",
    "if not data.empty:\n",
    "    # Perform the Time-Based Split into 3 Buckets\n",
    "    X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw = robust_time_split(\n",
    "        data, WINDOW_SIZE, STEP_SIZE, val_ratio=0.15, test_ratio=0.15\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTotal Windows: {len(X_train_raw) + len(X_val_raw) + len(X_test_raw)}\")\n",
    "    print(f\"Train Windows: {len(X_train_raw)}\")\n",
    "    print(f\"Val Windows:   {len(X_val_raw)}\")\n",
    "    print(f\"Test Windows:  {len(X_test_raw)}\")\n",
    "\n",
    "    # Encode Labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    # Fit on all labels to ensure we know all classes\n",
    "    all_labels = np.concatenate([y_train_raw, y_val_raw, y_test_raw])\n",
    "    label_encoder.fit(all_labels)\n",
    "    \n",
    "    class_names = label_encoder.classes_\n",
    "    num_classes = len(class_names)\n",
    "    print(\"Label Mapping:\", dict(zip(range(num_classes), class_names)))\n",
    "    # Convert to One-Hot Encoding\n",
    "    y_train = to_categorical(label_encoder.transform(y_train_raw))\n",
    "    y_val = to_categorical(label_encoder.transform(y_val_raw))\n",
    "    y_test = to_categorical(label_encoder.transform(y_test_raw))\n",
    "\n",
    "else:\n",
    "    print(\"No data loaded. Please check your .db files.\")\n",
    "    exit()\n",
    "\n",
    "# Plot Raw Counts\n",
    "def plot_class_distribution(y_train, y_val, y_test, classes):\n",
    "    train_indices = np.argmax(y_train, axis=1)\n",
    "    val_indices = np.argmax(y_val, axis=1)\n",
    "    test_indices = np.argmax(y_test, axis=1)\n",
    "\n",
    "    plot_data = []\n",
    "    for split_name, indices in [(\"Train\", train_indices), (\"Validation\", val_indices), (\"Test\", test_indices)]:\n",
    "        counts = pd.Series(indices).value_counts().sort_index()\n",
    "        for class_idx in range(len(classes)):\n",
    "            count = counts.get(class_idx, 0)\n",
    "            plot_data.append({\n",
    "                \"Class\": classes[class_idx],\n",
    "                \"Split\": split_name,\n",
    "                \"Count\": count\n",
    "            })\n",
    "            \n",
    "    df_plot = pd.DataFrame(plot_data)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    ax = sns.barplot(data=df_plot, x=\"Class\", y=\"Count\", hue=\"Split\", palette=\"muted\")\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fontsize=10)\n",
    "    \n",
    "    plt.title(\"Class Distribution (Raw Sample Counts)\", fontsize=14)\n",
    "    plt.ylabel(\"Number of Samples\", fontsize=12)\n",
    "    plt.xlabel(\"Activity Label\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Dataset Split\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- GENERATING DISTRIBUTION PLOT ---\")\n",
    "plot_class_distribution(y_train, y_val, y_test, class_names)\n",
    "\n",
    "# Normalization\n",
    "X_train_flat = X_train_raw.reshape(-1, 3)\n",
    "means = np.mean(X_train_flat, axis=0)\n",
    "stds = np.std(X_train_flat, axis=0)\n",
    "\n",
    "print(f\"\\nNormalization Stats:\")\n",
    "print(f\"Means: {means}\")\n",
    "print(f\"Stds:  {stds}\")\n",
    "\n",
    "X_train = (X_train_raw - means) / stds\n",
    "X_val = (X_val_raw - means) / stds\n",
    "X_test = (X_test_raw - means) / stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. PLOT AVERAGE WAVEFORMS PER CLASS\n",
    "# ==========================================\n",
    "def plot_average_waveforms(X, y_encoded, classes):\n",
    "    num_classes = len(classes)\n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(12, 4 * num_classes), sharex=True)\n",
    "    \n",
    "    if num_classes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    time_steps = np.arange(X.shape[1])\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        # Filter all windows belonging to this class\n",
    "        class_indices = np.where(y_encoded == i)[0]\n",
    "        class_windows = X[class_indices]\n",
    "        \n",
    "        # Calculate the mean across all windows for this class (axis 0)\n",
    "        # Result shape: (WINDOW_SIZE, 3)\n",
    "        mean_waveform = np.mean(class_windows, axis=0)\n",
    "        \n",
    "        # Plot X, Y, Z\n",
    "        ax = axes[i]\n",
    "        ax.plot(time_steps, mean_waveform[:, 0], label='X-axis', color='r', alpha=0.8)\n",
    "        ax.plot(time_steps, mean_waveform[:, 1], label='Y-axis', color='g', alpha=0.8)\n",
    "        ax.plot(time_steps, mean_waveform[:, 2], label='Z-axis', color='b', alpha=0.8)\n",
    "        \n",
    "        ax.set_title(f\"Average Waveform: {class_name} (n={len(class_indices)})\")\n",
    "        ax.set_ylabel(\"Amplitude\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.xlabel(\"Time Steps (Window Size)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- PLOTTING AVERAGE WAVEFORMS ---\")\n",
    "plot_average_waveforms(X_train_raw, np.argmax(y_train, axis=1), class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66853187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. MODEL TRAINING (1D CNN)\n",
    "# ==========================================\n",
    "model = Sequential([\n",
    "    Conv1D(filters=16, kernel_size=3, activation='relu', input_shape=(WINDOW_SIZE, 3)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Watch the validation loss\n",
    "    min_delta=0.001,\n",
    "    patience=10,         # Stop if it doesn't improve for 10 epochs\n",
    "    restore_best_weights=True # Go back to the best model found\n",
    ")\n",
    "\n",
    "print(\"\\n--- TRAINING MODEL ---\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# SAVE ORIGINAL H5 MODEL\n",
    "h5_path = os.path.join(OUTPUT_FOLDER, \"model.h5\")\n",
    "model.save(h5_path)\n",
    "print(f\"Original model saved to: {h5_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. PLOT TRAINING HISTORY\n",
    "# ==========================================\n",
    "def plot_training_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy', marker='o')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='o')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss', marker='o')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss', marker='o')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- PLOTTING TRAINING CURVES ---\")\n",
    "plot_training_history(history)\n",
    "\n",
    "# ==========================================\n",
    "# 7. EVALUATION\n",
    "# ==========================================\n",
    "print(\"\\n--- EVALUATION ON TEST SET ---\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
    "\n",
    "# ==========================================\n",
    "# 8. PLOT CONFUSION MATRIX\n",
    "# ==========================================\n",
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- PLOTTING CONFUSION MATRIX ---\")\n",
    "plot_confusion_matrix(y_true_classes, y_pred_classes, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 9. EXPORT & SIZE COMPARISON\n",
    "# ==========================================\n",
    "print(\"\\n--- EXPORTING MODELS & COMPARING SIZES ---\")\n",
    "\n",
    "# Define paths\n",
    "float_model_path = os.path.join(OUTPUT_FOLDER, \"model_float32.tflite\")\n",
    "quant_model_path = os.path.join(OUTPUT_FOLDER, \"model_quantized.tflite\")\n",
    "\n",
    "# Export Float32 Model (Non-Quantized)\n",
    "converter_float = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_float_model = converter_float.convert()\n",
    "\n",
    "with open(float_model_path, \"wb\") as f:\n",
    "    f.write(tflite_float_model)\n",
    "\n",
    "# Export Int8 Model (Quantized)\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(100):\n",
    "        yield [tf.cast(input_value, tf.float32)]\n",
    "\n",
    "converter_quant = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter_quant.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_quant.representative_dataset = representative_data_gen\n",
    "converter_quant.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter_quant.inference_input_type = tf.int8\n",
    "converter_quant.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_model = converter_quant.convert()\n",
    "\n",
    "with open(quant_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "# Calculate and Print Sizes\n",
    "h5_size = os.path.getsize(h5_path)\n",
    "float_size = os.path.getsize(float_model_path)\n",
    "quant_size = os.path.getsize(quant_model_path)\n",
    "reduction_h5_to_float = (1 - (float_size / h5_size)) * 100\n",
    "reduction_h5_to_int = (1 - (quant_size / h5_size)) * 100\n",
    "reduction_float_to_int = (1 - (quant_size / float_size)) * 100\n",
    "\n",
    "print(f\"\\nModel Size Comparison:\")\n",
    "print(f\"  H5 Model:   {h5_size / 1024:.2f} KB\")\n",
    "print(f\"  Float32 Model:   {float_size / 1024:.2f} KB\")\n",
    "print(f\"  Quantized Model: {quant_size / 1024:.2f} KB\")\n",
    "print(f\"  Reduction (TFlite Convert): {reduction_h5_to_float:.2f}%\")\n",
    "print(f\"  Reduction (TFlite Convert + quantisation): {reduction_h5_to_int:.2f}%\")\n",
    "print(f\"  Reduction (quantisation): {reduction_float_to_int:.2f}%\")\n",
    "\n",
    "# ============================================\n",
    "# 10. Generate C++ Header for Validation Data\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n--- GENERATING C++ HEADER (validation_data.h) ---\")\n",
    "\n",
    "num_samples = 5\n",
    "indices = np.random.choice(len(X_test), num_samples)\n",
    "samples_raw = X_test_raw[indices]\n",
    "true_labels = y_test[indices]\n",
    "\n",
    "cpp_content = f\"\"\"\n",
    "#ifndef VALIDATION_DATA_H\n",
    "#define VALIDATION_DATA_H\n",
    "\n",
    "// Auto-generated validation data\n",
    "// Class Map: {dict(zip(range(num_classes), class_names))}\n",
    "\n",
    "const int num_validation_samples = {num_samples};\n",
    "\n",
    "// Raw Input Data (Before Normalization)\n",
    "const int16_t validation_samples[{num_samples}][{WINDOW_SIZE}][3] = {{\n",
    "\"\"\"\n",
    "\n",
    "for sample in samples_raw:\n",
    "    cpp_content += \"    {\\n\"\n",
    "    for row in sample:\n",
    "        cpp_content += f\"        {{ {int(row[0])}, {int(row[1])}, {int(row[2])} }},\\n\"\n",
    "    cpp_content += \"    },\\n\"\n",
    "\n",
    "cpp_content += \"};\\n\\n\"\n",
    "cpp_content += f\"// Expected Classes (Indices): {np.argmax(true_labels, axis=1).tolist()}\\n\"\n",
    "cpp_content += \"#endif // VALIDATION_DATA_H\\n\"\n",
    "\n",
    "header_path = os.path.join(OUTPUT_FOLDER, \"validation_data.h\")\n",
    "with open(header_path, \"w\") as f:\n",
    "    f.write(cpp_content)\n",
    "\n",
    "print(\"Saved 'validation_data.h'\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
